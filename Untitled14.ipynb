{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOylNgnZ4cIKKAMqUciJReJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ravideswal03/Ai-chatbot/blob/main/Untitled14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vsjz5jXZKPgz"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from itertools import pairwise\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from accelerate import Accelerator\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from types import SimpleNamespace\n",
        "from typing import Optional"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Schedule:\n",
        "    '''Diffusion noise schedules parameterized by sigma'''\n",
        "    def __init__(self, sigmas: torch.FloatTensor):\n",
        "        self.sigmas = sigmas\n",
        "\n",
        "    def __getitem__(self, i) -> torch.FloatTensor:\n",
        "        return self.sigmas[i]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.sigmas)\n",
        "\n",
        "    def sample_sigmas(self, steps: int) -> torch.FloatTensor:\n",
        "        '''Called during sampling to get a decreasing sigma schedule with a\n",
        "        specified number of sampling steps:\n",
        "          - Spacing is \"trailing\" as in Table 2 of https://arxiv.org/abs/2305.08891\n",
        "          - Includes initial and final sigmas\n",
        "            i.e. len(schedule.sample_sigmas(steps)) == steps + 1\n",
        "        '''\n",
        "        indices = list((len(self) * (1 - np.arange(0, steps)/steps))\n",
        "                       .round().astype(np.int64) - 1)\n",
        "        return self[indices + [0]]\n",
        "\n",
        "    def sample_batch(self, x0: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        '''Called during training to get a batch of randomly sampled sigma values\n",
        "        '''\n",
        "        batchsize = x0.shape[0]\n",
        "        return self[torch.randint(len(self), (batchsize,))].to(x0)\n",
        "\n",
        "def sigmas_from_betas(betas: torch.FloatTensor):\n",
        "    return (1/torch.cumprod(1.0 - betas, dim=0) - 1).sqrt()\n",
        "\n",
        "# Simple log-linear schedule works for training many diffusion models\n",
        "class ScheduleLogLinear(Schedule):\n",
        "    def __init__(self, N: int, sigma_min: float=0.02, sigma_max: float=10):\n",
        "        super().__init__(torch.logspace(math.log10(sigma_min), math.log10(sigma_max), N))\n",
        "\n",
        "# Default parameters recover schedule used in most diffusion models\n",
        "class ScheduleDDPM(Schedule):\n",
        "    def __init__(self, N: int=1000, beta_start: float=0.0001, beta_end: float=0.02):\n",
        "        super().__init__(sigmas_from_betas(torch.linspace(beta_start, beta_end, N)))\n",
        "\n",
        "# Default parameters recover schedule used in most latent diffusion models, e.g. Stable diffusion\n",
        "class ScheduleLDM(Schedule):\n",
        "    def __init__(self, N: int=1000, beta_start: float=0.00085, beta_end: float=0.012):\n",
        "        super().__init__(sigmas_from_betas(torch.linspace(beta_start**0.5, beta_end**0.5, N)**2))\n",
        "\n",
        "# Sigmoid schedule used in GeoDiff\n",
        "class ScheduleSigmoid(Schedule):\n",
        "    def __init__(self, N: int=1000, beta_start: float=0.0001, beta_end: float=0.02):\n",
        "        betas = torch.sigmoid(torch.linspace(-6, 6, N)) * (beta_end - beta_start) + beta_start\n",
        "        super().__init__(sigmas_from_betas(betas))\n",
        "\n",
        "# Cosine schedule used in Nichol and Dhariwal 2021\n",
        "class ScheduleCosine(Schedule):\n",
        "    def __init__(self, N: int=1000, beta_start: float=0.0001, beta_end: float=0.02, max_beta: float=0.999):\n",
        "        alpha_bar = lambda t: np.cos((t + 0.008) / 1.008 * np.pi / 2) ** 2\n",
        "        betas = [min(1 - alpha_bar((i+1)/N)/alpha_bar(i/N), max_beta)\n",
        "                 for i in range(N)]\n",
        "        super().__init__(sigmas_from_betas(torch.tensor(betas, dtype=torch.float32)))\n",
        "\n",
        "# Given a batch of data x0, returns:\n",
        "#   eps  : i.i.d. normal with same shape as x0\n",
        "#   sigma: uniformly sampled from schedule, with shape Bx1x..x1 for broadcasting\n",
        "def generate_train_sample(x0: torch.FloatTensor, schedule: Schedule):\n",
        "    sigma = schedule.sample_batch(x0)\n",
        "    while len(sigma.shape) < len(x0.shape):\n",
        "        sigma = sigma.unsqueeze(-1)\n",
        "    eps = torch.randn_like(x0)\n",
        "    return sigma, eps\n",
        "\n",
        "# Model objects\n",
        "# Always called with (x, sigma):\n",
        "#   If x.shape == [B, D1, ..., Dk], sigma.shape == [] or [B, 1, ..., 1].\n",
        "#   If sigma.shape == [], model will be called with the same sigma for each x0\n",
        "#   Otherwise, x[i] will be paired with sigma[i] when calling model\n",
        "# Have a `rand_input` method for generating random xt during sampling\n",
        "\n",
        "def training_loop(loader     : DataLoader,\n",
        "                  model      : nn.Module,\n",
        "                  schedule   : Schedule,\n",
        "                  accelerator: Optional[Accelerator] = None,\n",
        "                  epochs     : int = 10000,\n",
        "                  lr         : float = 1e-3):\n",
        "    accelerator = accelerator or Accelerator()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    model, optimizer, loader = accelerator.prepare(model, optimizer, loader)\n",
        "    for _ in (pbar := tqdm(range(epochs))):\n",
        "        for x0 in loader:\n",
        "            optimizer.zero_grad()\n",
        "            sigma, eps = generate_train_sample(x0, schedule)\n",
        "            loss = model.get_loss(x0, sigma, eps)\n",
        "            yield SimpleNamespace(**locals()) # For extracting training statistics\n",
        "            accelerator.backward(loss)\n",
        "            optimizer.step()\n",
        "\n",
        "# Generalizes most commonly-used samplers:\n",
        "#   DDPM       : gam=1, mu=0.5\n",
        "#   DDIM       : gam=1, mu=0\n",
        "#   Accelerated: gam=2, mu=0\n",
        "@torch.no_grad()\n",
        "def samples(model      : nn.Module,\n",
        "            sigmas     : torch.FloatTensor, # Iterable with N+1 values for N sampling steps\n",
        "            gam        : float = 1.,        # Suggested to use gam >= 1\n",
        "            mu         : float = 0.,        # Requires mu in [0, 1)\n",
        "            xt         : Optional[torch.FloatTensor] = None,\n",
        "            accelerator: Optional[Accelerator] = None,\n",
        "            batchsize  : int = 1):\n",
        "    accelerator = accelerator or Accelerator()\n",
        "    if xt is None:\n",
        "        xt = model.rand_input(batchsize).to(accelerator.device) * sigmas[0]\n",
        "    else:\n",
        "        batchsize = xt.shape[0]\n",
        "    eps = None\n",
        "    for i, (sig, sig_prev) in enumerate(pairwise(sigmas)):\n",
        "        eps, eps_prev = model.predict_eps(xt, sig.to(xt)), eps\n",
        "        eps_av = eps * gam + eps_prev * (1-gam)  if i > 0 else eps\n",
        "        sig_p = (sig_prev/sig**mu)**(1/(1-mu)) # sig_prev == sig**mu sig_p**(1-mu)\n",
        "        eta = (sig_prev**2 - sig_p**2).sqrt()\n",
        "        xt = xt - (sig - sig_p) * eps_av + eta * model.rand_input(batchsize).to(xt)\n",
        "        yield xt"
      ],
      "metadata": {
        "id": "Z6qxWZacOCFa"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}